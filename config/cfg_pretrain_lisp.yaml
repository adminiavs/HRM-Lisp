# Lisp simplification training config

defaults:
  - arch: hrm_v1
  - _self_

hydra:
  output_subdir: null

# Increase reasoning depth for Lisp tasks
H_cycles: 4
L_cycles: 4

# Data path for the Lisp dataset
data_path: data/lisp-simplification

# Hyperparams - Training
global_batch_size: 256
epochs: 15000
eval_interval: 1000
checkpoint_every_eval: True

lr: 1e-4
lr_min_ratio: 0.1
lr_warmup_steps: 1000

# Standard hyperparameter settings for LM
beta1: 0.9
beta2: 0.95
weight_decay: 0.1
puzzle_emb_weight_decay: 0.1

# Hyperparams - Puzzle embeddings training (not used for Lisp, but kept for compatibility)
puzzle_emb_lr: 1e-2

# Early stopping
early_stopping_metric: accuracy
early_stopping_mode: max
early_stopping_patience: 5
early_stopping_min_delta: 0.0
early_stopping_set: null

# NOTE: Update 'vocab_size' in config/arch/hrm_v1.yaml with the output from
# the dataset generation script.